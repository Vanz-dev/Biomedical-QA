{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad55324e",
   "metadata": {},
   "source": [
    "# Biomedical Question Answering with PubMedQA and PubMedBERT\n",
    "\n",
    "This project implements a biomedical question answering pipeline using the PubMedQA dataset and PubMedBERT.\n",
    "\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "The code was developed and tested in a Unix-based environment (Ubuntu/Mac), consistent with the class VM configuration. All dependencies required to run the notebook are listed in the accompanying requirements.txt file. These includes libraries such as transformers, datasets, nltk, scikit-learn, sentence-transformers, and rank_bm25, as well as the accelerate library (version ≥ 0.26.0). To set up the environment, one can simply run pip install -r requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7031d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi        \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329d994",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "\n",
    "The dataset used is the PubMedQA dataset, which is a biomedical question-answering dataset designed for reasoning over biomedical research texts. This project utilizes the PubMedQA Labeled (PQA-L) subset, which contains 1000 manually expert annotations in the form of labels: yes/no/maybe.\n",
    "\n",
    "\n",
    "#### Loading and preparing the dataset\n",
    "\n",
    "First the pubmed_qa dataset is loaded from Hugging Face's dataset library, selecting the \"train\" split. We then shuffle the data using a fixed seed (42) to ensure consistent shuffling across runs.\n",
    "\n",
    "Next, we apply a train-test split, allocating 80% of the data for training and 20% for testing. Finally, we extract the train and test datasets from the split dictionary, storing them separately for later use in model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19038be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— load & split\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")[\"train\"].shuffle(seed=42)\n",
    "split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset, test_dataset = split[\"train\"], split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746f304",
   "metadata": {},
   "source": [
    "## BM25 retriever\n",
    "\n",
    "BM25 is a probabilistic ranking function commonly used in information retrieval. It is used to select relevant contexts for answering questions here. \n",
    "\n",
    "1. Prepare the corpus: created by extracting and joining contexts from the train_dataset. \n",
    "2. Initializing BM25 Tokenizer: A word tokenizer(TreebankWordTokenizer) is instantiated. This tokenizer breaks down each document into individual tokens for BM25 processing.\n",
    "3. Create BM25 index: BM25Okapi builds an index from tokenized documents. This allows scoring query relevance.\n",
    "4. retrieve_with_bm25 function: This function retrieves the top-k relevant contexts for a given query\n",
    "\n",
    "This implementation allows efficient retrieval of relevant biomedical contexts using BM25 ranking. The top-ranked documents help the question-answering model select the most useful information for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ce9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— BM25 retriever (unchanged)\n",
    "corpus = [' '.join(e['contexts']) for e in train_dataset[\"context\"]]\n",
    "tokenizer_bm25 = TreebankWordTokenizer()\n",
    "bm25 = BM25Okapi([tokenizer_bm25.tokenize(doc) for doc in corpus])\n",
    "def retrieve_with_bm25(q, k=1):\n",
    "    tokens = tokenizer_bm25.tokenize(q)\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "    return [corpus[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0112c",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To prepare biomedical question-answering data for model training, the preprocessing stage involves encoding categorical labels into numerical values for efficient classification.\n",
    "\n",
    "A dictionary (label2id) maps answers \"yes,\" \"no,\" and \"maybe\" to numeric identifiers (1, 0, and 2, respectively). An inverse mapping (id2label) is also created to convert predicted numeric labels back into their original categorical format, ensuring interpretability when reviewing model predictions.\n",
    "\n",
    "A domain-specific tokenizer is initialized using the pretrained PubMedBERT model, which is optimized for biomedical texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a68d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— preprocessing\n",
    "label2id = {'no': 0, 'yes': 1, 'maybe': 2}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989dab8",
   "metadata": {},
   "source": [
    "The preprocess function prepares biomedical question-answering data for model training. It tokenizes each question along with a retrieved relevant context using PubMedBERT. If no context is found, the entry is skipped. The function stores input IDs, attention masks, and numerical labels, ensuring compatibility with the model. It is then applied to both train and test datasets for structured input preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285ca93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    in_ids, attn, labs = [], [], []\n",
    "    for q, lbl in zip(examples['question'], examples['final_decision']):\n",
    "        docs = retrieve_with_bm25(q)\n",
    "        if not docs: continue\n",
    "        enc = tokenizer(q, docs[0], truncation=True, padding='max_length', max_length=512)\n",
    "        in_ids.append(enc['input_ids'])\n",
    "        attn.append(enc['attention_mask'])\n",
    "        labs.append(label2id[lbl.lower()])\n",
    "    return {'input_ids':in_ids, 'attention_mask':attn, 'labels':labs}\n",
    "\n",
    "train_enc = preprocess(train_dataset)\n",
    "test_enc  = preprocess(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece69dfe",
   "metadata": {},
   "source": [
    "The PubMedQADataset class is a custom dataset wrapper designed for use with PyTorch’s Dataset module. It takes preprocessed tokenized data (enc) as input and makes it compatible with PyTorch's data-loading pipeline.\n",
    "\n",
    "1. The __init__ method initializes the dataset by storing the encoded input dictionary.\n",
    "2. The __len__ method returns the number of samples based on the length of the labels list, ensuring proper iteration during training.\n",
    "3. The __getitem__ method retrieves a specific data sample as a dictionary, converting each element (input IDs, attention masks, labels) into PyTorch tensors for model compatibility.\n",
    "\n",
    "Finally, train_ds and test_ds instances are created, preparing data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c59848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubMedQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc): self.enc = enc\n",
    "    def __len__(self): return len(self.enc['labels'])\n",
    "    def __getitem__(self, i): return {k:torch.tensor(v[i]) for k,v in self.enc.items()}\n",
    "\n",
    "train_ds, test_ds = PubMedQADataset(train_enc), PubMedQADataset(test_enc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f87d83",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The compute_metrics function evaluates the performance of a biomedical question-answering model\n",
    "\n",
    "It extracts predicted labels by selecting the highest probability class using np.argmax(). Then, it computes accuracy, which measures overall correctness, and macro F1-score, which evaluates balance across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c6401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— classification metrics\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc   = accuracy_score(pred.label_ids, preds)\n",
    "    f1    = f1_score(pred.label_ids, preds, average='macro')\n",
    "    return {'accuracy':acc, 'f1':f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2b2dd",
   "metadata": {},
   "source": [
    "### Semantic Similarity Evaluation with Sentence-BERT\n",
    "\n",
    "#### Setting the Random Seed\n",
    "Before executing the model, we set a fixed random seed to ensure reproducibility.\n",
    "\n",
    "#### Defining the Semantic Similarity Evaluation Function\n",
    "\n",
    "The function evaluate_semantic_similarity calculates how similar predicted responses are to the actual reference answers.\n",
    "\n",
    "1. preds: A list of predicted answers from the model.\n",
    "\n",
    "2. refs: A list of reference (ground truth) answers.\n",
    "\n",
    "To achieve this, Sentence-BERT (all-MiniLM-L6-v2), a pretrained model designed for sentence embeddings, is used.\n",
    "\n",
    "The function iterates through the predicted-reference pairs, encoding each sentence into an embedding tensor and computing cosine similarity between them. Each similarity score is appended to a list, and after processing all pairs, an average similarity score is calculated and printed. \n",
    "\n",
    "This metric quantifies how closely the model’s predictions align with the expected answers, offering insight into its performance in capturing semantic nuances. By leveraging contextual embeddings and cosine similarity, this function provides a better measure of model evaluation in biomedical question answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538eadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(32)\n",
    "def evaluate_semantic_similarity(preds, refs):\n",
    "    model_Sentence = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    sim_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        sim = util.cos_sim(\n",
    "            model_Sentence.encode(pred, convert_to_tensor=True),\n",
    "            model_Sentence.encode(ref, convert_to_tensor=True)\n",
    "        ).item()\n",
    "        sim_scores.append(sim)\n",
    "    avg_sim = sum(sim_scores) / len(sim_scores)\n",
    "    print(f\"Average Semantic Similarity: {avg_sim:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64969572",
   "metadata": {},
   "source": [
    "### Pretraining\n",
    "\n",
    "The pretrained evaluation process assesses the baseline performance of the PubMedBERT model before fine-tuning.\n",
    "\n",
    "First, the pretrained model is loaded using AutoModelForSequenceClassification, specifying three classification labels (yes, no, maybe). The Trainer is then initialized, linking the model with the evaluation dataset (test_ds) and defining performance metrics (compute_metrics). The evaluation is executed using .evaluate(), which returns accuracy and macro F1 scores.\n",
    "\n",
    "After the initial assessment, the function extracts raw predictions by applying .predict() on the test dataset. The predicted labels are determined using np.argmax(), selecting the highest probability class for each instance. Reference labels are retrieved for comparison. Both predicted and reference labels are converted back to human-readable text using the predefined id2label mapping.\n",
    "\n",
    "Finally, the script evaluates the semantic similarity between the predicted and reference texts using evaluate_semantic_similarity(). This step measures how close the model’s raw outputs are to the expected answers, providing further insights into the pretrained model’s effectiveness before fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b79be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PART 1: Pretrained evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\dell\\Desktop\\QA_info539\\Biomedical-QA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2734869718551636, 'eval_model_preparation_time': 0.0041, 'eval_accuracy': 0.175, 'eval_f1': 0.15578002244668912, 'eval_runtime': 141.1092, 'eval_samples_per_second': 1.417, 'eval_steps_per_second': 0.177}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\Desktop\\QA_info539\\Biomedical-QA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity (pretrained):\n",
      "Average Semantic Similarity: 0.59\n"
     ]
    }
   ],
   "source": [
    "# — PART 1: pretrained evaluation\n",
    "print(\"=== PART 1: Pretrained evaluation ===\")\n",
    "pre_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    num_labels=3, id2label=id2label, label2id=label2id\n",
    ")\n",
    "pre_trainer = Trainer(model=pre_model, compute_metrics=compute_metrics, eval_dataset=test_ds)\n",
    "pre_res = pre_trainer.evaluate()\n",
    "print(pre_res)\n",
    "\n",
    "# get raw preds & refs as texts\n",
    "pred_out = pre_trainer.predict(test_ds)\n",
    "pred_ids = np.argmax(pred_out.predictions, axis=1)\n",
    "ref_ids  = pred_out.label_ids\n",
    "pred_texts = [id2label[i] for i in pred_ids]\n",
    "ref_texts  = [id2label[i] for i in ref_ids]\n",
    "\n",
    "print(\"Semantic similarity (pretrained):\")\n",
    "evaluate_semantic_similarity(pred_texts, ref_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1268fcd",
   "metadata": {},
   "source": [
    "### Fine tuning\n",
    "\n",
    "The fine-tuning and re-evaluation process aims to improve the biomedical question-answering model.\n",
    "\n",
    "\n",
    "First, we reload the pretrained model and define the number of classification labels (num_labels=3). Training hyperparameters, such as learning rate (2e-5), batch size (8), number of epochs (3), and weight decay (0.01), are configured using TrainingArguments. These settings help optimize model training while preventing overfitting.\n",
    "\n",
    "\n",
    "Next, the model is fine-tuned using the Trainer class, incorporating both the training dataset (train_ds) and evaluation metrics (compute_metrics). The .train() function initiates the fine-tuning process, adjusting model weights based on biomedical question-answering examples. Once training is complete, .evaluate() is used to assess model performance on the test dataset (test_ds), providing accuracy and macro F1-score results.\n",
    "\n",
    "After evaluation, predictions are extracted using .predict(), converting raw model outputs into categorical labels using np.argmax(). These predictions are mapped back to their textual representations (yes, no, maybe) via the id2label dictionary. Finally, semantic similarity between model predictions and reference answers is calculated using evaluate_semantic_similarity(), assessing how well the fine-tuned model captures meaning compared to ground truth responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36314dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PART 2: Finetune & evaluate ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\dell\\Desktop\\QA_info539\\Biomedical-QA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 1:40:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.782900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\Desktop\\QA_info539\\Biomedical-QA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.138144850730896, 'eval_accuracy': 0.475, 'eval_f1': 0.3110209601081812, 'eval_runtime': 132.0452, 'eval_samples_per_second': 1.515, 'eval_steps_per_second': 0.189, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\Desktop\\QA_info539\\Biomedical-QA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity (finetuned):\n",
      "Average Semantic Similarity: 0.83\n"
     ]
    }
   ],
   "source": [
    "# — PART 2: finetune & re-evaluate\n",
    "print(\"\\n=== PART 2: Finetune & evaluate ===\")\n",
    "ft_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    num_labels=3, id2label=id2label, label2id=label2id\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3, weight_decay=0.01, logging_steps=50\n",
    ")\n",
    "ft_trainer = Trainer(\n",
    "    model=ft_model, args=training_args,\n",
    "    train_dataset=train_ds, compute_metrics=compute_metrics\n",
    ")\n",
    "ft_trainer.train()\n",
    "ft_res = ft_trainer.evaluate(eval_dataset=test_ds)\n",
    "print(ft_res)\n",
    "\n",
    "# get finetuned preds & refs\n",
    "pred_out2 = ft_trainer.predict(test_ds)\n",
    "pred_ids2 = np.argmax(pred_out2.predictions, axis=1)\n",
    "ref_ids2  = pred_out2.label_ids\n",
    "pred_texts2 = [id2label[i] for i in pred_ids2]\n",
    "ref_texts2  = [id2label[i] for i in ref_ids2]\n",
    "\n",
    "print(\"Semantic similarity (finetuned):\")\n",
    "evaluate_semantic_similarity(pred_texts2, ref_texts2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d570c7",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The final comparison highlights the improvements achieved through fine-tuning. \n",
    "\n",
    "The accuracy of the fine-tuned model significantly increases from 0.1750 to 0.4750, showing better overall correctness in predictions. \n",
    "\n",
    "The macro F1-score also improves from 0.1558 to 0.3110, indicating enhanced balance across all classification labels. \n",
    "\n",
    "Additionally, the semantic similarity between predicted and reference answers shows a notable boost—from 0.59 to 0.83—suggesting that the fine-tuned model generates responses much closer to the expected answers in meaning. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8cf5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison on test set ===\n",
      "Accuracy → pretrained: 0.1750, finetuned: 0.4750\n",
      "   F1    → pretrained: 0.1558, finetuned: 0.3110\n",
      "Average semantic similarity -> pretrained:\n",
      "Average Semantic Similarity: 0.59\n",
      "Average semantic similarity -> finetuned:\n",
      "Average Semantic Similarity: 0.83\n"
     ]
    }
   ],
   "source": [
    "# — FINAL comparison\n",
    "print(\"\\n=== Comparison on test set ===\")\n",
    "print(f\"Accuracy → pretrained: {pre_res['eval_accuracy']:.4f}, finetuned: {ft_res['eval_accuracy']:.4f}\")\n",
    "print(f\"   F1    → pretrained: {pre_res['eval_f1']:.4f}, finetuned: {ft_res['eval_f1']:.4f}\")\n",
    "\n",
    "print(\"Average semantic similarity -> pretrained:\")\n",
    "semantic_pretrained = evaluate_semantic_similarity(pred_texts, ref_texts)\n",
    "\n",
    "print(\"Average semantic similarity -> finetuned:\")\n",
    "semantic_finetuned = evaluate_semantic_similarity(pred_texts2, ref_texts2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
