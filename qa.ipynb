{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08fb4b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9b4ff",
   "metadata": {},
   "source": [
    "<h2>Loading the dataset</h2>\n",
    "\n",
    "The PubMedQA dataset is loaded, specifically the pqa_labeled dataset that consists of 1000 samples. The dataset is shuffled and split into train and test set. \n",
    "\n",
    "The train set consists of 800 samples and the test set consists of 200 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c66b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")[\"train\"].shuffle(seed=42)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "#Three subsets of data for model training and evaluation\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f6611",
   "metadata": {},
   "source": [
    "This dataset is structured to support question-answering (QA) tasks related to biomedical literature. It contains five columns that provide essential information for each entry.\n",
    "\n",
    "1. pubid: A unique identifier for each record.\n",
    "2. question: The medical or scientific question posed.\n",
    "3. context: Background information related to the question.\n",
    "4. long_answer: A detailed response to the question based on the provided context.\n",
    "5. final_decision: yes/no/maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658767e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae8e93",
   "metadata": {},
   "source": [
    "<h2>BM25 for document retrieval</h2>\n",
    "\n",
    "Preparing a corpus from training set to be used with BM25, an efficient (alternative to tf-idf) ranking algorithm for information retrieval. BM25 helps rank documents by their relevance to a given query, making it useful for QA tasks.\n",
    "\n",
    "The following code achieves the following:\n",
    "- Retrieves the context field from the training dataset (train_dataset).\n",
    "- Converts the extracted texts into a list of strings (corpus).\n",
    "- Joins multiple contexts within each entry into a single string.\n",
    "- Initializes the Treebank Word Tokenizer, which is optimized for English text.\n",
    "- Tokenizes each document in the corpus into a list of words\n",
    "- Creates a BM25 index using the tokenized corpus.\n",
    "- BM25 ranks documents based on the frequency and importance of words in a query.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0d3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for BM25 using training set\n",
    "\n",
    "\n",
    "# Extract just the 'context' field from the train set\n",
    "corpus_data = train_dataset[\"context\"]\n",
    "corpus = [' '.join(entry['contexts']) for entry in corpus_data]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenized_corpus = [tokenizer.tokenize(doc) for doc in corpus]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574bdd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_bm25(query, k=3):\n",
    "    tokenized_query = tokenizer.tokenize(query)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:k]\n",
    "    return [corpus[i] for i in top_k_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b2ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1:\n",
      "In primary and secondary prevention trials, statins have been shown to reduce the risk of stroke. In addition to lipid lowering, statins have a number of antiatherothrombotic and neuroprotective properties. In a preliminary observational study, we explored whether clinical outcome is improved in patients who are on treatment with statins when stroke occurs. We conducted a population-based case-referent study of 25- to 74-year-old stroke patients with, for each case of a patient who was on statin\n",
      "\n",
      "Doc 2:\n",
      "Type 2 diabetes may be present for several years before diagnosis, by which time many patients have already developed diabetic complications. Earlier detection and treatment may reduce this burden, but evidence to support this approach is lacking. Glycemic control and clinical and surrogate outcomes were compared for 5,088 of 5,102 U.K. Diabetes Prospective Study participants according to whether they had low (<140 mg/dl [<7.8 mmol/l]), intermediate (140 to<180 mg/dl [7.8 to<10.0 mmol/l]), or hi\n",
      "\n",
      "Doc 3:\n",
      "\"America's Best Hospitals,\" an influential list published annually by U.S. News and World Report, assesses the quality of hospitals. It is not known whether patients admitted to hospitals ranked at the top in cardiology have lower short-term mortality from acute myocardial infarction than those admitted to other hospitals or whether differences in mortality are explained by differential use of recommended therapies. Using data from the Cooperative Cardiovascular Project on 149,177 elderly Medica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Does aspirin reduce the risk of stroke?\"\n",
    "top_docs = retrieve_with_bm25(query)\n",
    "\n",
    "for i, doc in enumerate(top_docs, 1):\n",
    "    print(f\"Doc {i}:\\n{doc[:500]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b11c8",
   "metadata": {},
   "source": [
    "<h2>Load BioMedBERT for Biomedical Question Answering</h2>\n",
    "\n",
    "Load BioMedBERT, a specialized BERT model designed for biomedical text processing, and sets up a question-answering (QA) pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b8c5a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Create QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e89bef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: referent study\n"
     ]
    }
   ],
   "source": [
    "# Try using top retrieved doc\n",
    "context_doc = top_docs[0]\n",
    "result = qa_pipeline({\n",
    "    \"question\": query,\n",
    "    \"context\": context_doc\n",
    "})\n",
    "print(f\"Answer: {result['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97c2448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_answer_to_label(answer: str) -> str:\n",
    "    answer = answer.lower()\n",
    "    if any(phrase in answer for phrase in [\"no\", \"not\", \"does not\", \"none\", \"negative\"]):\n",
    "        return \"no\"\n",
    "    elif any(phrase in answer for phrase in [\"yes\", \"does\", \"can\", \"reduce\", \"associated with\", \"positive\", \"increased\", \"decreased\"]):\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return \"maybe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7eba5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def evaluate(dataset, k=3, max_samples=100):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    examples = []\n",
    "\n",
    "    for example in random.sample(list(dataset), min(max_samples, len(dataset))):\n",
    "        question = example[\"question\"]\n",
    "        gold_label = example[\"final_decision\"]\n",
    "\n",
    "        # Retrieve top documents\n",
    "        retrieved_docs = retrieve_with_bm25(question, k=k)\n",
    "        if not retrieved_docs:\n",
    "            continue\n",
    "\n",
    "        # Use top-1 doc for answer extraction\n",
    "        context = retrieved_docs[0]\n",
    "\n",
    "        try:\n",
    "            result = qa_pipeline({\"question\": question, \"context\": context})\n",
    "            predicted_answer = result[\"answer\"]\n",
    "            predicted_label = map_answer_to_label(predicted_answer)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        if predicted_label == gold_label:\n",
    "            correct += 1\n",
    "\n",
    "        examples.append({\n",
    "            \"question\": question,\n",
    "            \"gold_label\": gold_label,\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"context_snippet\": context[:300]\n",
    "        })\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac85980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.17\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_examples = evaluate(test_dataset, max_samples=100)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8667c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
