{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08fb4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9b4ff",
   "metadata": {},
   "source": [
    "<h2>Loading the dataset</h2>\n",
    "\n",
    "The PubMedQA dataset is loaded, specifically the pqa_labeled dataset that consists of 1000 samples. The dataset is shuffled and split into train, development, and test set. \n",
    "\n",
    "The train set consists of 450 samples. The dev set consists of 50 samples and the test set consists of 500 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c66b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")[\"train\"].shuffle(seed=42)\n",
    "split_dataset = dataset.train_test_split(test_size=500, seed=42)\n",
    "split_dataset[\"dev\"] = split_dataset[\"train\"].train_test_split(test_size=50, seed=42)[\"test\"]\n",
    "\n",
    "#Three subsets of data for model training and evaluation\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "dev_dataset = split_dataset[\"dev\"]\n",
    "test_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f6611",
   "metadata": {},
   "source": [
    "This dataset is structured to support question-answering (QA) tasks related to biomedical literature. It contains five columns that provide essential information for each entry.\n",
    "\n",
    "1. pubid: A unique identifier for each record.\n",
    "2. question: The medical or scientific question posed.\n",
    "3. context: Background information related to the question.\n",
    "4. long_answer: A detailed response to the question based on the provided context.\n",
    "5. final_decision: yes/no/maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d29b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae8e93",
   "metadata": {},
   "source": [
    "<h2>BM25 for document retrieval</h2>\n",
    "\n",
    "Preparing a corpus from training set to be used with BM25, an efficient (alternative to tf-idf) ranking algorithm for information retrieval. BM25 helps rank documents by their relevance to a given query, making it useful for QA tasks.\n",
    "\n",
    "The following code achieves the following:\n",
    "- Retrieves the context field from the training dataset (train_dataset).\n",
    "- Converts the extracted texts into a list of strings (corpus).\n",
    "- Joins multiple contexts within each entry into a single string.\n",
    "- Initializes the Treebank Word Tokenizer, which is optimized for English text.\n",
    "- Tokenizes each document in the corpus into a list of words\n",
    "- Creates a BM25 index using the tokenized corpus.\n",
    "- BM25 ranks documents based on the frequency and importance of words in a query.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb0d3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for BM25 using training set\n",
    "\n",
    "\n",
    "# Extract just the 'context' field from the train set\n",
    "corpus_data = train_dataset[\"context\"]\n",
    "corpus = [' '.join(entry['contexts']) for entry in corpus_data]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenized_corpus = [tokenizer.tokenize(doc) for doc in corpus]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f66081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rank_bm25.BM25Okapi at 0x7ff5b3561d00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c5a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
